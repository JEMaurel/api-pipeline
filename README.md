# API Pipe Docker ğŸš€

Este proyecto es un pipeline de datos (ETL) completamente dockerizado que extrae informaciÃ³n de una API de E-commerce, realiza transformaciones de limpieza y organiza un Data Lake local con particionamiento eficiente.

## ğŸ› ï¸ TecnologÃ­as utilizadas
* **Python 3.11**: LÃ³gica principal del pipeline.
* **Pandas**: Procesamiento y transformaciÃ³n de datos.
* **Docker & Docker Compose**: ContenerizaciÃ³n para asegurar la portabilidad.
* **Parquet**: Formato de almacenamiento optimizado para Big Data.
* **WSL2 (Ubuntu)**: Entorno de ejecuciÃ³n y desarrollo.

## ğŸ—ï¸ Arquitectura del Proyecto
El pipeline sigue una estructura de **Data Lake** profesional:
1. **ExtracciÃ³n**: ObtenciÃ³n de 1000 registros desde una API REST.
2. **TransformaciÃ³n**: Limpieza de columnas y tipado de datos.
3. **Carga (Storage)**: Los datos se guardan en formato `.parquet` usando un esquema de particionamiento **Hive-style** (`order_year=YYYY/order_month=YYYY-MM`).



## ğŸš€ CÃ³mo ejecutarlo

1. **Clonar el repositorio:**
   ```bash
   git clone [https://github.com/JEMaurel/api-pipe-docker.git](https://github.com/JEMaurel/api-pipe-docker.git)
   cd api-pipe-docker
Configurar variables de entorno: Crea un archivo .env en la raÃ­z con tus credenciales (puedes usar .env.example como guÃ­a):

Plaintext
API_TOKEN=tu_token
AWS_ACCESS_KEY_ID=tu_key
...
Ejecutar con Docker:

Bash
docker compose up --build
ğŸ“‚ Estructura de Salida
Al finalizar, los datos procesados se encuentran en la carpeta output/ organizados por tiempo, lo que permite consultas altamente eficientes.

output/orders/
â”œâ”€â”€ order_year=2024
â”‚   â””â”€â”€ order_month=2024-01
â”‚       â””â”€â”€ data.parquet
