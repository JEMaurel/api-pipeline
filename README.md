# ğŸš€ API Ecommerce ETL Pipeline

Este proyecto implementa un flujo robusto de **ExtracciÃ³n, TransformaciÃ³n y Carga (ETL)** diseÃ±ado para procesar datos de e-commerce de forma eficiente. El sistema consume informaciÃ³n de una API REST, aplica transformaciones con Pandas y persiste los resultados en un formato de alto rendimiento (**Parquet**) con particionamiento fÃ­sico.



---

## âœ¨ CaracterÃ­sticas Principales

- **Resiliencia:** LÃ³gica de reintentos (Retry strategy) integrada para manejar fallos temporales de red.
- **Dockerizado:** Entorno 100% reproducible mediante Docker y Docker Compose.
- **Almacenamiento Optimizado:** Uso de formato Parquet que reduce el espacio en disco y acelera las consultas analÃ­ticas.
- **Particionamiento AutomÃ¡tico:** OrganizaciÃ³n fÃ­sica de datos por aÃ±o y mes (`order_year`/`order_month`), optimizada para Data Lakes.
- **Seguridad:** GestiÃ³n de credenciales sensibles mediante variables de entorno (`.env`).

---

## ğŸ› ï¸ Requisitos Previos

- **Docker Desktop** (Recomendado) o **Python 3.11+** instalado localmente.
- ConexiÃ³n a internet para el consumo de la API.

---

## ğŸš€ GuÃ­a de Inicio RÃ¡pido con Docker

La forma mÃ¡s eficiente de ejecutar el pipeline es mediante contenedores:



1. **Configurar el entorno:**
   Crea un archivo llamado `.env` en la raÃ­z del proyecto con el siguiente contenido:
   ```env
   API_TOKEN=tu_token_aqui
   API_BASE_URL=[https://iansaura.com/api](https://iansaura.com/api)
   API_EMAIL=tu@email.com

2. Levantar el servicio
Desde tu terminal, ejecuta:

Bash

docker-compose up   

ğŸ› ï¸ InstalaciÃ³n Manual (Local)
Si prefieres trabajar en un entorno local sin utilizar Docker:

Bash

# Clonar el repositorio
git clone [https://github.com/JEMaurel/api-pipeline.git](https://github.com/JEMaurel/api-pipeline.git)
cd api-pipeline

# Crear y activar entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias y ejecutar
pip install -r requirements.txt
python main.py
ğŸ“Š Flujo del Pipeline
El proceso se ejecuta en tres etapas lÃ³gicas:

Extract: Consumo de la API ecommerce con validaciÃ³n de cÃ³digos de estado y manejo de excepciones.

Transform: Procesamiento avanzado con Pandas que incluye limpieza de nulos, tipado de datos y normalizaciÃ³n.

Load: Escritura en el directorio local /output utilizando el motor PyArrow.

ğŸ“‚ Estructura de Salida (Output)
Los archivos se organizan jerÃ¡rquicamente para facilitar su consumo en herramientas de BI o motores de SQL:

Plaintext

output/
â””â”€â”€ orders/
    â”œâ”€â”€ order_year=2024/
    â”‚   â””â”€â”€ order_month=01/
    â”‚       â””â”€â”€ part-0.parquet
    â””â”€â”€ order_year=2025/
        â””â”€â”€ order_month=01/
            â””â”€â”€ part-0.parquet

ğŸ§° TecnologÃ­as Utilizadas
Lenguaje: Python 3.11

Procesamiento de Datos: Pandas

Contenedores: Docker & Docker Compose

Formato de Archivos: Parquet (PyArrow)

Seguridad: Python-dotenv